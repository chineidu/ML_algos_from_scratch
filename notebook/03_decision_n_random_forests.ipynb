{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms Using Trees\n",
    "\n",
    "## Decision Trees (From Scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "[![image.png](https://i.postimg.cc/RCDcYtgh/image.png)](https://postimg.cc/5j8YYXdW)\n",
    "\n",
    "<br>\n",
    "\n",
    "[Source](https://www.youtube.com/watch?v=NxEHSAfFlK8&t=287)\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/nVBF8bkm/image.png)](https://postimg.cc/vD8F9KC8)\n",
    "\n",
    "\n",
    "### Decisions To Be Made\n",
    "\n",
    "1. **Split feature**: Which feature should be used for the splitting?\n",
    "2. **Split point**: At what point in a numerical variable should we split?\n",
    "3. **When to stop splitting**: When you should you stop splitting to avoid trees from growing so big?\n",
    "\n",
    "### Steps\n",
    "\n",
    "> The following steps are used to build the Decision Tree classifier from scratch.\n",
    "\n",
    "#### Training\n",
    "Given the entire dataset:\n",
    "1. Calculate the information gain with each possible split. i.e using all the possible features, calculate the IG.\n",
    "2. Divide the data with the feature and the value threshold (if it's numerical) that gives the most information gain.\n",
    "3. The result from step 2 is used to create the branches.\n",
    "4. Repeat steps 1 thru 3 until a stopping criteria is reached.\n",
    "\n",
    "#### Making Predictions\n",
    "Given a data point:\n",
    "\n",
    "1. Traverse the tree until you reach a leaf node.\n",
    "2. Return the most common class label i.e (if a leaf node is pure, return the class label otherwise, return a majority vote)\n",
    "\n",
    "\n",
    "#### Important Terms\n",
    "\n",
    "* Entropy: This refers to how much variance the data has. i.e. it measures how random or unpredictable a node is. The entropy is largest when a node has 50% of both classes (e.g. a binary class). It ranges between `0` and `1`.\n",
    "\n",
    "$$\n",
    "Entropy = - \\sum^C_{i=1}(p_{i}*log_{2}(p_{i}))\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$p_{i}$ is the probability of randomly picking an element of $class_{i}$ .\n",
    "\n",
    "$C$ is the total number of classes. For a binary problem, $C = 2$. i.e $C_{unique} = [0, 1]$\n",
    "\n",
    "[]\n",
    "\n",
    "* **Information Gain (IG)**: This measures the quality of the splits. i.e. it measures how much entropy was removed by splitting on a feature. It's the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node. It ranges between `0` and `1`.\n",
    "\n",
    "$$\n",
    "IG = Entropy_{parent} - (weighted_{average}* Entropy_{children})\n",
    "$$}\n",
    "\n",
    "where:\n",
    "\n",
    "$weighted_{average}* Entropy_{children}$: $((\\frac{num_{LeftNodes}}{total} * entropy_{Left}) + (\\frac{num_{RightNodes}}{total} * entropy_{Right}))$\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "1. **Maximum depth**: This refers to how deep you want the tree to grow.\n",
    "2. **Minimum no of samples**: Refers to the minimum number of samples a node can have before splitting can take place.\n",
    "3. **Minimum impurity decrease**: Refers to the minimum entropy change required for a split to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 classes. The 1st class (Node) is used to implement\n",
    "# the node and all its attributes while 2nd class (DecisionTree)\n",
    "# contains all the logic for the classifier. The DecisionTree has\n",
    "# the attributes used as stopping criteria which prevents the tree\n",
    "# from growing uncontrollably.\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"This class is used to implement the nodes\n",
    "    of a Decision Tree classifier.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        left: Union[float, int] = None,\n",
    "        right: Union[float, int] = None,\n",
    "        feature: Union[float, int] = None,\n",
    "        threshold: Union[float, int] = None,\n",
    "        *,\n",
    "        value: Union[float, int] = None,\n",
    "    ):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "\n",
    "    def _is_leaf_node(self) -> bool:\n",
    "        \"\"\"It returns True if it's a leaf node and False otherwise.\"\"\"\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "# Training\n",
    "# Given the entire dataset:\n",
    "# 1. Calculate the information gain with each possible split.\n",
    "# i.e using all the possible features, calculate the IG.\n",
    "# 2. Divide the data with the feature and the value threshold (if it's numerical)\n",
    "# that gives the most information gain.\n",
    "# 3. The result from step 2 is used to create the branches (grow the tree)\n",
    "# a. check the stopping criteria to prevent growing trees uncontrollably.\n",
    "# b. find the best split using IG.\n",
    "# c. create child nodes.\n",
    "# 4. Repeat steps 1 thru 3 until a stopping criteria is reached.\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"This class is used to implement Decision Tree classifier.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_depth: int, min_samples_split: int, n_features: Optional[int] = None\n",
    "    ) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"This is used to train the model.\"\"\"\n",
    "        # check that the n_features is valid.\n",
    "        all_feats = X.shape[1]\n",
    "        self.n_features = (\n",
    "            all_feats if self.n_features is None else min(all_feats, self.n_features)\n",
    "        )\n",
    "        # grow trees\n",
    "        self.root = self._grow_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def _grow_tree(self, X: np.ndarray, y: np.ndarray, max_depth: int = 0):\n",
    "        \"\"\"This is used to create child nodes recursively.\"\"\"\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_K = np.unique(y)  # number of unique labels.\n",
    "\n",
    "        # Base case: check the stopping criteria: if the n_K = 1\n",
    "        # or if the n_samples < min_samples_split or depth > max_depth\n",
    "        # and return the only present label or the label with\n",
    "        # the label with the highest frequency.\n",
    "        if (\n",
    "            n_K == 1\n",
    "            or n_samples <= self.min_samples_split\n",
    "            or max_depth >= self.max_depth\n",
    "        ):\n",
    "            leaf_node = DecisionTree._get_most_common_label(input_=y)\n",
    "            return Node(value=leaf_node)\n",
    "\n",
    "        # Randomly select features (indices)\n",
    "        feature_idxs = np.random.choice(n_feats, size=self.n_features, replace=False)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_thresh = self._best_split(X, y, feature_idxs)\n",
    "\n",
    "        # Create child nodes (recursively) using the result from the best_split\n",
    "        left_idxs, right_idxs = DecisionTree._split(\n",
    "            feat=X[:, best_feature], split_thresh=best_thresh\n",
    "        )\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], max_depth=max_depth + 1)\n",
    "        right = self._grow_tree(\n",
    "            X[right_idxs, :], y[right_idxs], max_depth=max_depth + 1\n",
    "        )\n",
    "        return Node(left, right, best_feature, best_feature)\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_split(\n",
    "        X: np.ndarray, y: np.ndarray, feature_idxs: list[int]\n",
    "    ) -> tuple[float, int]:\n",
    "        \"\"\"This uses information gain to calculate the best split at\n",
    "        every node. It returns the feature index and the label of\n",
    "        the feeature to split on.\n",
    "\n",
    "        Returns:\n",
    "            split_idx, split_thresh: The feature index and feature\n",
    "            label respectively.\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        split_idx, split_thresh = None, None\n",
    "        best_gain = -1\n",
    "\n",
    "        # Calculate the IG for each feature and determine the best\n",
    "        for feat_idx in feature_idxs:\n",
    "            feat = X[:, feat_idx]\n",
    "            thresholds = np.unique(feat)  # Unique labels\n",
    "\n",
    "            for thresh in thresholds:\n",
    "                # Calculate IG\n",
    "                info_gain = DecisionTree._calculate_info_gain(y)\n",
    "                # Update values\n",
    "                if info_gain > best_gain:\n",
    "                    best_gain = info_gain\n",
    "                    split_idx, split_thresh = feat_idx, thresh\n",
    "\n",
    "        return (split_idx, split_thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_info_gain(y: np.ndarray) -> float:\n",
    "        \"\"\"This is used to calculate the information gain.\n",
    "        It ranges between 0 and 1.\"\"\"\n",
    "        # Calculate entropy of the parent\n",
    "        parent_entropy = DecisionTree._calculate_entropy(y)\n",
    "\n",
    "        # Create children i.e split into left and right branches\n",
    "        left_idxs, right_idxs = DecisionTree._split()\n",
    "\n",
    "        # If the left or right nodes is empty. i.e. after the split,\n",
    "        # there are nodes with no class labels, info_gain=0\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            info_gain = 0\n",
    "\n",
    "        # Calculate weighted average: using the number of labels in the\n",
    "        # left and right nodes and left and right entropies.\n",
    "        num_left_nodes, num_right_nodes = len(left_idxs), len(right_idxs)\n",
    "        left_entropy, right_entropy = DecisionTree._calculate_entropy(\n",
    "            y[left_idxs]\n",
    "        ), DecisionTree._calculate_entropy(y[right_idxs])\n",
    "\n",
    "        # Calculate the entropy of the children\n",
    "        child_entropy = (num_left_nodes / len(y) * left_entropy) + (\n",
    "            num_right_nodes / len(y) * right_entropy\n",
    "        )\n",
    "\n",
    "        # Calculate the IG\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_entropy(input_: Union[list[int], np.ndarray]) -> float:\n",
    "        \"\"\"This is used to calculate the entropy.\"\"\"\n",
    "        counts = np.bincount(input_)\n",
    "        probs = counts / len(input_)\n",
    "        entropy = -np.sum([(p_k * np.log2(p_k)) for p_k in probs if p_k > 0])\n",
    "        return entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_most_common_label(input_: np.ndarray) -> int:\n",
    "        \"\"\"This returns the most common label.\"\"\"\n",
    "        counter = Counter(input_)\n",
    "        return counter.most_common(n=1)[0][0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _split(feat, split_thresh) -> tuple[list, list]:\n",
    "        \"\"\"This is used to split a node into the left and right branches.\n",
    "        It returns a tuple of lists.\n",
    "        \"\"\"\n",
    "        # Return the idxs that satisfy the condition\n",
    "        left_idxs = np.argwhere(feat <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(feat > split_thresh).flatten()\n",
    "        return (left_idxs, right_idxs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "IG = Entropy_{parent} - (weighted_{average}* Entropy_{children})\n",
    "$$}\n",
    "\n",
    "where:\n",
    "\n",
    "$weighted_{average}* Entropy_{children}$: $((\\frac{num_{LeftNodes}}{total} * entropy_{Left}) + (\\frac{num_{RightNodes}}{total} * entropy_{Right}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2023 - 1937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([0, 1, 2, 3, 4, 5])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(arr > 3).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(6).reshape(2, 3)\n",
    "\n",
    "print(x)\n",
    "np.argwhere(x > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612366047228759"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
    "entropies = []\n",
    "K, total = np.unique(a), len(a)\n",
    "counts = Counter(a)\n",
    "\n",
    "for k_ in K:\n",
    "    p_k = counts[k_] / total\n",
    "    entropy = p_k * np.log2(p_k)\n",
    "    entropies.append(entropy)\n",
    "E = -np.sum(entropies)\n",
    "\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612366047228759"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
    "counts = np.bincount(a)\n",
    "probs = counts / len(a)\n",
    "entropy = -np.sum([(p_k * np.log2(p_k)) for p_k in probs if p_k > 0])\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612366047228759"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.sum([((counts[k_] / total) * np.log2(counts[k_] / total)) for k_ in K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.15384615, 0.15384615, 0.07692308])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = np.bincount([1, 2, 3, 1, 2])\n",
    "prob = hist / total\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.9 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cfa5e2e7f6473d0731a0f2d805e3c50a81965be55a72eefbad345a8551b801f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
