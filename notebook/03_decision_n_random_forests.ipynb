{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms Using Trees\n",
    "\n",
    "## Decision Trees (From Scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from run_algos import utils\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "[![image.png](https://i.postimg.cc/RCDcYtgh/image.png)](https://postimg.cc/5j8YYXdW)\n",
    "\n",
    "<br>\n",
    "\n",
    "[Source](https://www.youtube.com/watch?v=NxEHSAfFlK8&t=287)\n",
    "\n",
    "<br>\n",
    "\n",
    "[![image.png](https://i.postimg.cc/nVBF8bkm/image.png)](https://postimg.cc/vD8F9KC8)\n",
    "\n",
    "\n",
    "### Decisions To Be Made\n",
    "\n",
    "1. **Split feature**: Which feature should be used for the splitting?\n",
    "2. **Split point**: At what point in a numerical variable should we split?\n",
    "3. **When to stop splitting**: When you should you stop splitting to avoid trees from growing so big?\n",
    "\n",
    "### Steps\n",
    "\n",
    "> The following steps are used to build the Decision Tree classifier from scratch.\n",
    "\n",
    "#### Training\n",
    "Given the entire dataset:\n",
    "1. Calculate the information gain with each possible split. i.e using all the possible features, calculate the IG.\n",
    "2. Divide the data with the feature and the value threshold (if it's numerical) that gives the most information gain.\n",
    "3. The result from step 2 is used to create the branches.\n",
    "4. Repeat steps 1 thru 3 until a stopping criteria is reached.\n",
    "\n",
    "#### Making Predictions\n",
    "Given a data point:\n",
    "\n",
    "1. Traverse the tree until you reach a leaf node.\n",
    "2. Return the most common class label i.e (if a leaf node is pure, return the class label otherwise, return a majority vote)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Important Terms\n",
    "\n",
    "* Entropy: This refers to how much variance the data has. i.e. it measures how random or unpredictable a node is. The entropy is largest when a node has 50% of both classes (e.g. a binary class). It ranges between `0` and `1`.\n",
    "\n",
    "$$\n",
    "Entropy = - \\sum^C_{i=1}(p_{i}*log_{2}(p_{i}))\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$p_{i}$ is the probability of randomly picking an element of $class_{i}$ .\n",
    "\n",
    "$C$ is the total number of classes. For a binary problem, $C = 2$. i.e $C_{unique} = [0, 1]$\n",
    "\n",
    "[]\n",
    "\n",
    "* **Information Gain (IG)**: This measures the quality of the splits. i.e. it measures how much entropy was removed by splitting on a feature. It's the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node. It ranges between `0` and `1`.\n",
    "\n",
    "$$\n",
    "IG = Entropy_{parent} - (weighted_{average}* Entropy_{children})\n",
    "$$}\n",
    "\n",
    "where:\n",
    "\n",
    "$weighted_{average}* Entropy_{children}$: $((\\frac{num_{LeftNodes}}{total} * entropy_{Left}) + (\\frac{num_{RightNodes}}{total} * entropy_{Right}))$\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "1. **Maximum depth**: This refers to how deep you want the tree to grow.\n",
    "2. **Minimum no of samples**: Refers to the minimum number of samples a node can have before splitting can take place.\n",
    "3. **Minimum impurity decrease**: Refers to the minimum entropy change required for a split to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, NewType, Union\n",
    "\n",
    "Tree = NewType(\"Tree\", tp=Any)\n",
    "\n",
    "# Create 2 classes. The 1st class (Node) is used to implement\n",
    "# the node and all its attributes while 2nd class (DecisionTree)\n",
    "# contains all the logic for the classifier. The DecisionTree has\n",
    "# the attributes used as stopping criteria which prevents the tree\n",
    "# from growing uncontrollably.\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"This class is used to implement the nodes\n",
    "    of a Decision Tree classifier.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        left: Union[float, int] = None,\n",
    "        right: Union[float, int] = None,\n",
    "        feature: Union[float, int] = None,\n",
    "        threshold: Union[float, int] = None,\n",
    "        *,\n",
    "        value: Union[float, int] = None,\n",
    "    ):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "\n",
    "    def _is_leaf_node(self) -> bool:\n",
    "        \"\"\"It returns True if it's a leaf node and False otherwise.\"\"\"\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "# Training\n",
    "# Given the entire dataset:\n",
    "# 1. Calculate the information gain with each possible split.\n",
    "# i.e using all the possible features, calculate the IG.\n",
    "# 2. Divide the data with the feature and the value threshold (if it's numerical)\n",
    "# that gives the most information gain.\n",
    "# 3. The result from step 2 is used to create the branches (grow the tree)\n",
    "# a. check the stopping criteria to prevent growing trees uncontrollably.\n",
    "# b. find the best split using IG.\n",
    "# c. create child nodes.\n",
    "# 4. Repeat steps 1 thru 3 until a stopping criteria is reached.\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"This class is used to implement Decision Tree classifier.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: int = 100,\n",
    "        min_samples_split: int = 2,\n",
    "        n_features: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"This returns the string representation of the class.\"\"\"\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(max_depth={self.max_depth}, \"\n",
    "            f\"min_samples_split={self.min_samples_split}, \"\n",
    "            f\"n_features={self.n_features}, \"\n",
    "            f\"root={self.root})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"This is used to train the model.\"\"\"\n",
    "        # check that the n_features is valid.\n",
    "        all_feats = X.shape[1]\n",
    "        self.n_features = (\n",
    "            all_feats if self.n_features is None else min(all_feats, self.n_features)\n",
    "        )\n",
    "        # grow trees\n",
    "        self.root = self._grow_tree(X, y)\n",
    "        return self\n",
    "\n",
    "    def _grow_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\"This is used to create child nodes recursively.\n",
    "\n",
    "        It returns a Node object\n",
    "        \"\"\"\n",
    "        n_samples, n_feats = X.shape\n",
    "        n_K = len(np.unique(y))  # number of unique labels.\n",
    "\n",
    "        # Base case: check the stopping criteria: if the n_K = 1\n",
    "        # or if the n_samples < min_samples_split or depth > max_depth\n",
    "        # and return the only present label or the label with\n",
    "        # the label with the highest frequency.\n",
    "        if n_K == 1 or n_samples < self.min_samples_split or depth >= self.max_depth:\n",
    "            leaf_node = DecisionTree._get_most_common_label(input_=y)\n",
    "            return Node(value=leaf_node)\n",
    "\n",
    "        # Randomly select features (indices)\n",
    "        feature_idxs = np.random.choice(n_feats, size=self.n_features, replace=False)\n",
    "\n",
    "        # Find the best split: Select the feature and the\n",
    "        # label of the feature used for splitting\n",
    "        best_feature, best_thresh = self._best_split(X, y, feature_idxs)\n",
    "\n",
    "        # Create child nodes (recursively) using the result from the best_split\n",
    "        left_idxs, right_idxs = DecisionTree._split_into_nodes(\n",
    "            feat_matrix=X[:, best_feature], split_thresh=best_thresh\n",
    "        )\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth=depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth=depth + 1)\n",
    "        return Node(left, right, best_feature, best_thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_split(\n",
    "        X: np.ndarray, y: np.ndarray, feature_idxs: list[int]\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"This uses information gain to calculate the best split at\n",
    "        every node. It returns the feature index and the label of\n",
    "        the feeature to split on.\n",
    "\n",
    "        Returns:\n",
    "            feat_split_idx, label_split_thresh: The best feature index\n",
    "            and feature label respectively used to perform the split.\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        feat_split_idx, label_split_thresh = None, None\n",
    "        best_gain = -1\n",
    "\n",
    "        # Calculate the IG for each feature and determine the best\n",
    "        for feat_idx in feature_idxs:\n",
    "            feat_matrix = X[:, feat_idx]  # Matrix (2-D)\n",
    "            thresholds = np.unique(feat_matrix)  # Unique labels of the feature\n",
    "\n",
    "            for thresh in thresholds:\n",
    "                # Calculate the Info Gain\n",
    "                info_gain = DecisionTree._calculate_info_gain(y, feat_matrix, thresh)\n",
    "                # Update values\n",
    "                if info_gain > best_gain:\n",
    "                    best_gain = info_gain\n",
    "                    feat_split_idx, label_split_thresh = feat_idx, thresh\n",
    "\n",
    "        return (feat_split_idx, label_split_thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_info_gain(\n",
    "        y: np.ndarray, feat_matrix: np.ndarray, split_thresh: int\n",
    "    ) -> float:\n",
    "        \"\"\"This is used to calculate the information gain.\n",
    "        It ranges between 0 and 1.\"\"\"\n",
    "        # Calculate entropy of the parent\n",
    "        parent_entropy = DecisionTree._calculate_entropy(y)\n",
    "\n",
    "        # Create children i.e split into left and right nodes\n",
    "        left_idxs, right_idxs = DecisionTree._split_into_nodes(\n",
    "            feat_matrix, split_thresh\n",
    "        )\n",
    "\n",
    "        # If the left or right nodes is empty. i.e. after the split,\n",
    "        # there are nodes with no class labels, info_gain=0\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            info_gain = 0\n",
    "\n",
    "        # Calculate weighted average: using the number of labels in the\n",
    "        # left and right nodes and left and right entropies.\n",
    "        num_left_nodes, num_right_nodes = len(left_idxs), len(right_idxs)\n",
    "        left_entropy, right_entropy = DecisionTree._calculate_entropy(\n",
    "            y[left_idxs]\n",
    "        ), DecisionTree._calculate_entropy(y[right_idxs])\n",
    "\n",
    "        # Calculate the entropy of the children\n",
    "        child_entropy = (num_left_nodes / len(y) * left_entropy) + (\n",
    "            num_right_nodes / len(y) * right_entropy\n",
    "        )\n",
    "\n",
    "        # Calculate the IG\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_entropy(input_: Union[list[int], np.ndarray]) -> float:\n",
    "        \"\"\"This is used to calculate the entropy.\"\"\"\n",
    "        counts = np.bincount(input_)\n",
    "        probs = counts / len(input_)\n",
    "        entropy = -np.sum([(p_k * np.log2(p_k)) for p_k in probs if p_k > 0])\n",
    "        return entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_most_common_label(input_: np.ndarray) -> int:\n",
    "        \"\"\"This returns the most common label.\"\"\"\n",
    "        counter = Counter(input_)\n",
    "        return counter.most_common(n=1)[0][0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_into_nodes(\n",
    "        feat_matrix: np.ndarray, split_thresh: int\n",
    "    ) -> tuple[list, list]:\n",
    "        \"\"\"This is used to split a node into the left and right nodes.\n",
    "        It returns a tuple of lists.\n",
    "\n",
    "        Params:\n",
    "            feat_matrix (np.ndarray): A 2-D array (Matrix)\n",
    "            split_thresh (int):\n",
    "        \"\"\"\n",
    "        # Return the idxs that satisfy the condition\n",
    "        left_idxs = np.argwhere(feat_matrix <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(feat_matrix > split_thresh).flatten()\n",
    "        return (left_idxs, right_idxs)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"This is used to make inference on the entire data.\"\"\"\n",
    "        pred = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(pred)\n",
    "\n",
    "    def _traverse_tree(self, x, node: Node) -> int:\n",
    "        \"\"\"This is used to traverse recursively through the tree.\"\"\"\n",
    "        # Base case: Check if it's a leaf node\n",
    "        if node._is_leaf_node():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        # if x[node.feature] > node.threshold\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 11), (200, 11))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = utils.generate_mock_data(type_=\"classification\")\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=utils.TEST_SIZE, random_state=utils.RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTree(max_depth=5, min_samples_split=100, n_features=11, root=<__main__.Node object at 0x132758c70>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_tree = DecisionTree(max_depth=5, min_samples_split=100)\n",
    "d_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = d_tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "np.mean(y_pred == y_test)\n",
    "\n",
    "# np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "[![image.png](https://i.postimg.cc/1R82hKM8/image.png)](https://postimg.cc/PNk21YgH)\n",
    "\n",
    "### Overview\n",
    "* This is a collection of many Decision Trees (hence `forest`).\n",
    "* A **subset** of the data is chosen at **`random`** (hence `random`), and a decision tree is used to make predictions based on the subset of data chosen.\n",
    "* The process is repeated a `N` number of times, where `N` is the number of decision trees in the forest.\n",
    "* The predictions made by all of the decision trees in the forest are used to make the final prediction by majority voting at inference.\n",
    "  * Classification: majority vote.\n",
    "  * Regression: mean of the predictions.\n",
    "\n",
    "<hr><br>\n",
    "\n",
    "### Training\n",
    "1. A subset of the data is chosen at random with replacement, resulting in some data points being repeated and not all of the actual data being used for training (bootstrapping).\n",
    "2. Based on the subset of data selected, `N` decision trees are used to fit (train).\n",
    "\n",
    "### Inference (Making Predictions)\n",
    "1. The trained decision trees are used to make predictions.\n",
    "2. For classification, the predictions made by all of the decision trees in the forest are used to make the final prediction by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Init hyperparams\n",
    "# * n_trees, * max_depth, * min_samples_split, * n_features, * root\n",
    "# 2. Create N number of decision trees\n",
    "# 3. Bootstrap the samples (the training data) with repetition so that not\n",
    "# all of the training data is used.\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    \"\"\"This class is used to implement the Random Forest classifier.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_trees: int = 20,\n",
    "        max_depth: int = 100,\n",
    "        min_samples_split: int = 2,\n",
    "        n_features: int = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(min_num_sample={self.min_samples_split} \"\n",
    "            f\"max_depth={self.max_depth}, \"\n",
    "            f\"n_features={self.n_features}, \"\n",
    "            f\"n_trees={self.n_trees})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.trees = []\n",
    "        for _ in np.arange(self.n_trees):\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                n_features=self.n_features,\n",
    "            )\n",
    "            X_sampled, y_sampled = self._booststrap(X=X, y=y)\n",
    "            # Fit and append\n",
    "            tree.fit(X_sampled, y_sampled)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _booststrap(*, X: np.ndarray, y: np.ndarray) -> tuple[list[int], list[int]]:\n",
    "        \"\"\"This returns random samples from the data having the\n",
    "        same size as the training data.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        # With replace=True ensures that not all the samples are chosen\n",
    "        # because a few samples will be repeated and chosen_sample == n_samples\n",
    "        chosen_samples = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return (X[chosen_samples, :], y[chosen_samples])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_most_common_label(*, input_: np.ndarray) -> int:\n",
    "        \"\"\"This returns the most common label.\"\"\"\n",
    "        counter = Counter(input_)\n",
    "        return counter.most_common(n=1)[0][0]\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # This returns the predicted labels for each data point per tree.\n",
    "        # i.e tree_0_pred, tree_1_pred, tree_2_pred, ...\n",
    "        # [[0,1,1], [1,1,0], [0,0,1], ...]\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # But what we actually want are all the predictions by the trees\n",
    "        # for each data point in a single array.\n",
    "        # e.g. [[0,1,0] [1,1,0], [1,0,1], ...]\n",
    "        predictions = np.swapaxes(tree_preds, axis1=0, axis2=1)\n",
    "        predictions = [self._get_most_common_label(input_=pred) for pred in predictions]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForest(max_depth=50)\n",
    "\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_clf.predict(X=X_test)\n",
    "\n",
    "# Accuracy\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.random_forest import RandomForest\n",
    "\n",
    "clf = RandomForest()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X=X_test)\n",
    "\n",
    "# Accuracy\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cfa5e2e7f6473d0731a0f2d805e3c50a81965be55a72eefbad345a8551b801f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
