{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression From Scratch\n",
    "\n",
    "* `Linear regression` is an algorithm that provides a linear relationship between an independent (predictor) variable and a dependent (target) variable to predict the outcome of future events.\n",
    "* Thus, `linear regression` is a supervised learning algorithm that predicts continuous or numeric variables such as sales, salary, age, product price, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations\n",
    "\n",
    "### Predicted (Estimated) Value\n",
    "\n",
    "$$ \n",
    "\\hat{y} = wX + b\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\hat{y}_{i} = wx_{i} + b\n",
    "$$\n",
    "\n",
    "\n",
    "where \n",
    "* `y_hat` is the **estimated value**,\n",
    "*  `w` is the **weight**, \n",
    "*  `X` is the **predictor** \n",
    "*  and `b` is the **bias**.\n",
    "\n",
    "### Loss\n",
    "\n",
    "* Mean Square Error (MSE): This the the average (mean) difference between the actual and estimated value. The goal is to minimize the loss on every iteration.\n",
    "\n",
    "$$ \n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N}({y_{i} - \\hat{y}_{i}})^2\n",
    "$$\n",
    "\n",
    "* **Loss Function**: This computes the error for a **single training example**.\n",
    "* **Cost Function**: This computes the **average** of the loss functions for the **entire training set**. \n",
    "\n",
    "### Weight and Bias\n",
    "\n",
    "* Weight: This is the coefficient of the predictor variable. They're the values that are used to multiply the predictor values.\n",
    "\n",
    "* Bias: This is a parameter in linear regression that allows us to `learn` a shifted fuction. i.e it allows us to shift the straight-line up an down.\n",
    "\n",
    "\n",
    "### Training with Gradient Descent\n",
    "\n",
    "`Gradient Descent` is an optimization algorithm which tries to **iteratively tweak** the **parameters** of a model (weights and biases) in order to find the set of parameter values that **minimizes** the model's **prediction error** (cost function).\n",
    "\n",
    "* After the parameters of the model have been initialised randomly, each iteration of gradient descent goes as follows: \n",
    "  * with the given values of such parameters, we use the model to make a prediction for every instance of the training data, and compare that prediction to the actual target value.\n",
    "\n",
    "  * Once we have computed this aggregated error (known as cost function), we measure the local gradient of this error with respect to the model parameters, and update these parameters by pushing them in the direction of descending gradient, thus making the cost function decrease.\n",
    "\n",
    "Source: [Here](https://towardsdatascience.com/linear-regression-explained-d0a1068accb9)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "* In order for Gradient Descent to work, we must set the `learning rate` to an appropriate value. Using a good learning rate is crucial.\n",
    "\n",
    "* The `learning rate` determines how fast or slow we will move towards the optimal weights. \n",
    "  * If the learning rate is **very large**, we will skip the optimal solution. \n",
    "  * If it is **too small**, we will need too many iterations to converge to the best values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "* **Differentiation of the weight (dw)**\n",
    "$$\n",
    "\\frac{df}{dw} = dw = \\frac{1}{N}\\sum_{i=1}^{N}-2x_{i}({y_{i} - \\hat{y}_{i})} = \\frac{1}{N}\\sum_{i=1}^{N}2x_{i}({\\hat{y}_{i} - y_{i})}\n",
    "$$\n",
    "\n",
    "* **Differentiation of the bias (db)**\n",
    "\n",
    "$$\n",
    "\\frac{df}{db} = db = \\frac{1}{N}\\sum_{i=1}^{N}-2({y_{i} - \\hat{y}_{i})} = \\frac{1}{N}\\sum_{i=1}^{N}2({\\hat{y}_{i} - y_{i})}\n",
    "$$\n",
    "\n",
    "### Update Rules\n",
    "\n",
    "* Update the weights and biases\n",
    "\n",
    "$$\n",
    "w = w - \\alpha.dw\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha.db\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps For Building Linear Regression (From Scratch)\n",
    "\n",
    "1. Training: **Initialize** the weight and bias as **zero** (0).\n",
    "2. Given data points, **predict** (estimate) the result using:\n",
    "     $$\n",
    "      \\hat{y} = wX + b\n",
    "     $$\n",
    "3. calculate the **error** (loss function).\n",
    "4. **update** the parameters (weight and bias) using gradient descent until a minimal cost function is obtained.\n",
    "5. **repeat** the entire steps `n` times.\n",
    "6. Use the update parameters to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"This is an implementation of linear regression.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float = 0.001, n_iters: int = 1_000) -> None:\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{__class__.__name__}(learning_rate={self.learning_rate}, n_iters={self.n_iters:,})\"\n",
    "\n",
    "    def fit(self, X=np.ndarray, y=np.ndarray) -> None:\n",
    "        \"\"\"This is used to train the linear regression model.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Step1: Initialize the weight and bias\n",
    "        self.weight = np.zeros((n_features))  # Vector\n",
    "        self.bias = 0  # Scalar\n",
    "\n",
    "        # Step2: Estimate the y_value given the data points\n",
    "        # Note: shape of X: (n_samples, n_features) and shape of weight: (n_features, 1)\n",
    "        # Dot product: (A, B) x (B, C). i.e the inner dimensions MUST be equal.\n",
    "        # For more info check: https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "        for _ in range(self.n_iters):\n",
    "            y_pred = np.dot(X, self.weight) + self.bias\n",
    "\n",
    "            # Step3: Calculate the change in weight and bias values for each training\n",
    "            # example using gradient descent.\n",
    "            # shape of x_i: (1, n_features), shape of (y - y_hat): (1,) a rank1 array\n",
    "            # so we need to transpose x_i. Note that np.dot also performs the summation.\n",
    "            dw = (1 / n_samples) * 2 * (np.dot(X.T, (y_pred - y)))\n",
    "            db = (1 / n_samples) * 2 * np.sum(y_pred - y)\n",
    "\n",
    "            # Step4: Update the parameters\n",
    "            self.weight -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> float:\n",
    "        \"\"\"This is used to make predictions.\"\"\"\n",
    "        # Step5. Use the update parameters to make predictions.\n",
    "        y_pred = np.dot(X, self.weight) + self.bias\n",
    "        return y_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_MSE(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"This is used to calculate the mean square error.\"\"\"\n",
    "        mse = np.mean(np.square(y_true - y_pred))\n",
    "        return round(mse, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mse(y_true, y_pred):\n",
    "    mse = np.mean(np.square(y_true - y_pred))\n",
    "    return round(mse, 2)\n",
    "\n",
    "\n",
    "# # visualize the relationship between the predictor and the target\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(X[:, 0], y, s=20)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"Main program\"\"\"\n",
    "    # create a synthetic data\n",
    "    data = make_regression(n_samples=300, n_features=1, noise=27, random_state=4)\n",
    "    X, y = data\n",
    "\n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=4\n",
    "    )\n",
    "\n",
    "    # instantiate model\n",
    "    linear_reg = LinearRegression(learning_rate=0.001, n_iters=1_000)\n",
    "\n",
    "    # train model\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_pred = linear_reg.predict(X_test)\n",
    "\n",
    "    mse = cal_mse(y_test, y_pred)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464.83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "* Logistic Regression is used for solving classification problems.\n",
    "* It uses the `sigmoid` function to convert the predictions into a binary value (number between 0 and 1).\n",
    "\n",
    "* `Sigmoid` function is also called a `squashing` function as its domain is the set of all real numbers, and its range is (0, 1). Hence, if the input to the function is either a very large negative number or a very large positive number, the output is always between 0 and 1. Same goes for any number between -∞ and +∞.\n",
    "\n",
    "\n",
    "$$\n",
    "S(x) = \\frac{1}{1 + \\exp^{-x}}\n",
    "$$\n",
    "\n",
    "* For our usecase, this becomes:\n",
    "\n",
    "$$\n",
    "y_{pred}(x) = \\frac{1}{1 + \\exp^{-y_{linear}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "y_{linear} = wX + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "* **Differentiation of the weight (dw)**\n",
    "$$\n",
    "\\frac{df}{dw} = dw = \\frac{1}{N}\\sum_{i=1}^{N}-2x_{i}({y_{i} - \\hat{y}_{i})} = \\frac{1}{N}\\sum_{i=1}^{N}2x_{i}({\\hat{y}_{i} - y_{i})}\n",
    "$$\n",
    "\n",
    "* **Differentiation of the bias (db)**\n",
    "\n",
    "$$\n",
    "\\frac{df}{db} = db = \\frac{1}{N}\\sum_{i=1}^{N}-2({y_{i} - \\hat{y}_{i})} = \\frac{1}{N}\\sum_{i=1}^{N}2({\\hat{y}_{i} - y_{i})}\n",
    "$$\n",
    "\n",
    "### Update Rules\n",
    "\n",
    "* Update the weights and biases\n",
    "\n",
    "$$\n",
    "w = w - \\alpha.dw\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha.db\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_iters: int = 2_000, learning_rate: float = 0.001) -> None:\n",
    "        self.n_iters = n_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.THRESH = 0.5\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{__class__.__name__}(learning_rate={self.learning_rate}, \"\n",
    "            f\"n_iters={self.n_iters:,})\"\n",
    "        )\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # Initialize the parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weight = np.zeros((n_features))  # Vector\n",
    "        self.bias = 0  # Scalar\n",
    "\n",
    "        for _ in np.arange(self.n_iters):\n",
    "            # Make predictions. Convert the continuous variable \n",
    "            # to a number between 0 and 1.\n",
    "            y_hat = np.dot(X, self.weight) + self.bias\n",
    "            y_pred = self.__sigmoid(y_hat)\n",
    "\n",
    "            # Using Gradient descent, minimize the loss function\n",
    "            # i.e the loss for each training example\n",
    "            dw = (1 / n_samples) * 2 * (np.dot(X.T, (y_pred - y)))\n",
    "            db = 2 * np.mean(y_pred - y)\n",
    "\n",
    "            # Update the parameters\n",
    "            self.weight -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "        return self\n",
    "\n",
    "    def __sigmoid(self, y_hat: float) -> float:\n",
    "        \"\"\"This returns a number between 0 and 1.\"\"\"\n",
    "        _y_pred = 1 / (1 + np.exp(-y_hat))\n",
    "        return _y_pred\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"This is used to make predictions.\"\"\"\n",
    "        y_hat = np.dot(X, self.weight) + self.bias\n",
    "        _y_pred = self.__sigmoid(y_hat)\n",
    "        y_pred = [1 if val > self.THRESH else 0 for val in _y_pred]\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "RANDOM_STATE = 123\n",
    "TEST_SIZE = 0.1\n",
    "N_SAMPLES = 2_000\n",
    "N_FEATURES = 1\n",
    "N_CLASSES = 2\n",
    "NOISE = 10\n",
    "\n",
    "\n",
    "def generate_mock_data(*, type_: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"This generates the synthetic data required for classification\n",
    "    or regression.\n",
    "\n",
    "    Params:\n",
    "        type_ (str): 'classification' or 'regression'\n",
    "\n",
    "    Returns:\n",
    "        (X, y) (tuple): It returns the predictor and the target variable.\n",
    "    \"\"\"\n",
    "    type_value = [\"classification\", \"regression\"]\n",
    "    if type_ not in type_value:\n",
    "        raise ValueError(f\"{type_!r} should be {type_value[0]!r} or {type_value[1]!r}.\")\n",
    "\n",
    "    regres_data = make_regression(\n",
    "        n_samples=N_SAMPLES,\n",
    "        n_features=N_FEATURES,\n",
    "        noise=NOISE,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    classif_data = make_classification(\n",
    "        n_samples=N_SAMPLES,\n",
    "        n_features=N_FEATURES + 10,\n",
    "        n_classes=N_CLASSES,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    data = regres_data if type == \"regression\" else classif_data\n",
    "    X, y = data\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_mock_data(type_=\"classification\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression(n_iters=10_000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)\n",
    "y_pred\n",
    "print(np.mean(y_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee85a8c565f0f8759c7967b8f49958a69a05efec3a3965c7debede210912a7da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
